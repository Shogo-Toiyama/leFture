import shutil
import traceback
import json
from pathlib import Path
from datetime import datetime

# ä½œæˆã—ãŸè¨­å®šã¨Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
from app.core.config import JobStatus, PipelineSteps, PIPELINE_STEPS_NUM, BASE_WORK_DIR
from app.core.supabase import get_supabase_client
from app.services.helpers.helpers import print_log, init_logger, finalize_log_and_get_path
from app.services.helpers.llm_unified import UnifiedLLM, CostCollector

from app.services.logic.transcription import TranscriptionService
from app.services.logic.sentence_review import SentenceReviewService
from app.services.logic.role_classification import RoleClassificationService
from app.services.logic.lecture_segmentaion import LectureSegmentationService
from app.services.logic.topic_details_generation import TopicDetailGenerationService
from app.services.logic.fun_fact_generation import FunFactGenerationService


async def run_lecture_pipeline(job_id: str):
    """
    ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œã•ã‚Œã‚‹ãƒ¡ã‚¤ãƒ³å‡¦ç†ã€‚
    processing_jobs ãƒ†ãƒ¼ãƒ–ãƒ«ã® job_id ã‚’å—ã‘å–ã‚Šã€æœ€å¾Œã¾ã§å‡¦ç†ã‚’è¡Œã†ã€‚
    """
    supabase = get_supabase_client()
    
    # ä½œæ¥­ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ (/tmp/job_id/)
    work_dir = BASE_WORK_DIR / job_id
    if work_dir.exists():
        shutil.rmtree(work_dir)
    work_dir.mkdir(parents=True, exist_ok=True)

    # å…±é€šãƒ„ãƒ¼ãƒ«ã®åˆæœŸåŒ–
    init_logger(work_dir)
    llm = UnifiedLLM(provider="gemini") # å¿…è¦ã«å¿œã˜ã¦ openai ã«å¤‰æ›´
    collector = CostCollector()
    
    # æˆæœç‰©ã®ãƒ‘ã‚¹ã‚’ä¸€æ™‚ä¿å­˜ã™ã‚‹è¾æ›¸
    current_artifacts = {}

    try:
        print_log(f"ğŸš€ Job Started: {job_id}")

        # ---------------------------------------------------------
        # 0. Jobãƒ‡ãƒ¼ã‚¿ã®å–å¾— & Lectureæƒ…å ±ã®ç¢ºèª
        # ---------------------------------------------------------
        # Jobæƒ…å ±ã‚’å–å¾—
        job_res = supabase.table("processing_jobs").select("*").eq("id", job_id).single().execute()
        job_data = job_res.data
        lecture_id = job_data["lecture_id"]
        
        if job_data["status"] != JobStatus.PENDING or job_data["current_step"] != PipelineSteps.PENDING:
            print_log(f"Job is already executed. [Status: {job_data['status']}, Step: {job_data['current_step']}]")
            return

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ PROCESSING ã«å¤‰æ›´
        _update_job_progress(supabase, job_id, JobStatus.PROCESSING, "READY", current_artifacts)

        # Lectureãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
        lecture_res = supabase.table("lecture_assets").select("storage_path").eq("lecture_id", lecture_id).single().execute()
        storage_path = lecture_res.data["storage_path"]
        uid = storage_path.split("/", 1)[0]


        # ---------------------------------------------------------
        # 1. DOWNLOADING (éŸ³å£°ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰)
        # ---------------------------------------------------------
        step_name = PipelineSteps.DOWNLOADING
        _update_job_progress(supabase, job_id, JobStatus.PROCESSING, step_name, current_artifacts)
        
        local_audio_path = work_dir / "input_audio.m4a"
        
        # Supabase Storage ('lecture_assets') ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        with open(local_audio_path, "wb") as f:
            res = supabase.storage.from_("lecture_assets").download(storage_path)
            f.write(res)
            
        print_log(f"âœ… Downloaded: {local_audio_path}")


        # ---------------------------------------------------------
        # 2. TRANSCRIBING (æ–‡å­—èµ·ã“ã—)
        # ---------------------------------------------------------
        step_name = PipelineSteps.TRANSCRIBING
        _update_job_progress(supabase, job_id, JobStatus.PROCESSING, step_name, current_artifacts)

        transcriber = TranscriptionService(collector)
        transcript_path = transcriber.run(local_audio_path, work_dir)
        
        # æˆæœç‰©ã‚’Supabase Storageã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼†ãƒ‘ã‚¹è¨˜éŒ²
        remote_trans_path = _upload_artifact(supabase, uid, lecture_id, transcript_path, "transcript.json")
        current_artifacts["transcript_json"] = remote_trans_path


        # ---------------------------------------------------------
        # 3. SENTENCE_REVIEWING (æ–‡ç« æ ¡æ­£)
        # ---------------------------------------------------------
        step_name = PipelineSteps.SENTENCE_REVIEWING
        _update_job_progress(supabase, job_id, JobStatus.PROCESSING, step_name, current_artifacts)

        reviewer = SentenceReviewService(llm, collector)
        reviewed_paths = reviewer.run(transcript_path, work_dir)
        
        final_json_path = None

        for path in reviewed_paths:
            # ãƒ•ã‚¡ã‚¤ãƒ«åã§åˆ¤æ–­ã—ã¦æŒ¯ã‚Šåˆ†ã‘ã‚‹
            filename = path.name
            
            if filename == "reviewed_sentences_raw.json":
                # ã“ã‚Œã¯ã€Œé€”ä¸­çµŒé (Temp)ã€
                remote_path = _upload_artifact(supabase, uid, lecture_id, path, filename, isTemp=True)
                current_artifacts["reviewed_sentences_raw_json"] = remote_path
            
            elif filename == "reviewed_sentences.json":
                # ã“ã‚ŒãŒã€Œå®Œæˆå“ã€
                remote_path = _upload_artifact(supabase, uid, lecture_id, path, filename)
                current_artifacts["reviewed_sentences_json"] = remote_path
                final_json_path = path
                
            elif filename == "reviewed_sentences_raw_text.txt":
                # ã“ã‚Œã¯ã€Œå¤±æ•—æ™‚ã®ãƒ­ã‚° (Temp)ã€
                remote_path = _upload_artifact(supabase, uid, lecture_id, path, filename, isTemp=True)
                current_artifacts["reviewed_sentences_error_text"] = remote_path

        # ã‚‚ã—å®Œæˆå“(final_json_path)ãŒãªã‘ã‚Œã°ã€ã“ã“ã§ã‚¨ãƒ©ãƒ¼ã«ã™ã‚‹
        if not final_json_path:
            raise ValueError("Sentence Review failed to generate final JSON. Check temp artifacts for raw text.")



        # ---------------------------------------------------------
        # 4. ROLE CLASSIFICATION
        # ---------------------------------------------------------

        step_name = PipelineSteps.ROLE_CLASSIFYING
        _update_job_progress(supabase, job_id, JobStatus.PROCESSING, step_name, current_artifacts)

        role_classifier = RoleClassificationService(llm, collector)
        role_artifacts = await role_classifier.run(final_json_path, work_dir)

        final_sentences_path = None

        for path in role_artifacts:
            filename = path.name
            
            if filename == "sentences_final.json":
                remote_path = _upload_artifact(supabase, uid, lecture_id, path, "sentences_final.json")
                current_artifacts["sentences_final_json"] = remote_path
                final_sentences_path = path
                
            elif filename.endswith(".zip"):
                 remote_path = _upload_artifact(supabase, uid, lecture_id, path, filename, isTemp=True)
                 current_artifacts["role_batches_zip"] = remote_path

        if not final_sentences_path:
             raise ValueError("Role Classification failed to generate final JSON.")
        
        # ---------------------------------------------------------
        # 5. SEGMENTING (è©±é¡Œåˆ†å‰²)
        # ---------------------------------------------------------
        step_name = PipelineSteps.SEGMENTING
        _update_job_progress(supabase, job_id, JobStatus.PROCESSING, step_name, current_artifacts)

        segmenter = LectureSegmentationService(llm, collector)
        seg_paths = await segmenter.run(final_sentences_path, work_dir)

        final_segments_path = None
        for path in seg_paths:
            if path.name == "segments.json":
                current_artifacts["segments_json"] = _upload_artifact(supabase, uid, lecture_id, path, "segments.json")
                final_segments_path = path
        
        if not final_segments_path:
             # Segmentationå¤±æ•—ã—ã¦ã‚‚æ¬¡ã«é€²ã‚ãªã„
             raise ValueError("Segmentation failed.")
        
        # ---------------------------------------------------------
        # 6. GENERATING_DETAILS (è©³ç´°è§£èª¬ç”Ÿæˆ)
        # ---------------------------------------------------------
        step_name = PipelineSteps.GENERATING_DETAILS
        _update_job_progress(supabase, job_id, JobStatus.PROCESSING, step_name, current_artifacts)

        detailer = TopicDetailGenerationService(llm, collector)
        # æˆ»ã‚Šå€¤ã¯ [segments_with_details.json]
        detail_paths = await detailer.run(final_segments_path, final_sentences_path, work_dir)

        segments_with_details_path = None # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«æ¸¡ã™ç”¨

        for path in detail_paths:
            if path.name == "segments_with_details.json":
                # Tempã¨ã—ã¦ä¿å­˜ã—ã¦ã‚‚ã„ã„ã—ã€é‡è¦ãªä¸­é–“ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜ã—ã¦ã‚‚OK
                current_artifacts["segments_with_details_json"] = _upload_artifact(supabase, uid, lecture_id, path, "segments_with_details.json")
                segments_with_details_path = path


        # ---------------------------------------------------------
        # 7. GENERATING_FUN_FACTS (è±†çŸ¥è­˜ç”Ÿæˆ)
        # ---------------------------------------------------------
        step_name = PipelineSteps.GENERATING_FUN_FACTS
        _update_job_progress(supabase, job_id, JobStatus.PROCESSING, step_name, current_artifacts)

        # inputã¯ work_dir å†…ã® segments_with_details.json ã‚’å‹æ‰‹ã«è¦‹ã«è¡Œãä»•æ§˜ã«ã—ãŸã®ã§å¼•æ•°ã¯ work_dir ã ã‘ã§OK
        fun_facter = FunFactGenerationService(llm, collector)
        fun_paths = await fun_facter.run(work_dir, segments_with_details_path)

        for path in fun_paths:
            if path.name == "lecture_complete_data.json":
                # ã“ã‚ŒãŒã‚¢ãƒ—ãƒªã§ä½¿ã†ã€Œå®Œå…¨ç‰ˆãƒ‡ãƒ¼ã‚¿ã€ï¼
                current_artifacts["lecture_complete_data_json"] = _upload_artifact(supabase, uid, lecture_id, path, "lecture_complete_data.json")

        # ---------------------------------------------------------
        # X. COMPLETED (å®Œäº†å‡¦ç†)
        # ---------------------------------------------------------
        step_name = PipelineSteps.COMPLETED
        _update_job_progress(supabase, job_id, JobStatus.DONE, step_name, current_artifacts)
        
        # æœ€å¾Œã« lectures ãƒ†ãƒ¼ãƒ–ãƒ«ã® final_markdown_path ãªã©ã‚’æ›´æ–°ã—ã¦ã‚‚è‰¯ã„
        # supabase.table("lectures").update({...}).eq("id", lecture_id).execute()

        print_log(f"ğŸ‰ Job Completed Successfully: {job_id}")
        print_log(collector.report())


    except Exception as e:
        # ---------------------------------------------------------
        # ERROR HANDLING (å¤±æ•—æ™‚ã®å‡¦ç†)
        # ---------------------------------------------------------
        error_msg = f"{str(e)}\n{traceback.format_exc()}"
        print_log(f"âŒ Job Failed at {step_name}: {error_msg}")
        
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä½œæˆ
        error_data = {
            "message": str(error_msg),
            "step": step_name,
            "timestamp": datetime.now().isoformat(),
            "traceback": traceback.format_exc()
        }

        # DBæ›´æ–°: Status=ERROR, Step=å¤±æ•—ã—ãŸã‚¹ãƒ†ãƒƒãƒ—ã®ã¾ã¾
        supabase.table("processing_jobs").update({
            "status": JobStatus.ERROR,
            "error_message": json.dumps(error_data), # JSONBå¯¾å¿œ
            "updated_at": datetime.now().isoformat()
        }).eq("id", job_id).execute()

    finally:
        try:
            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ•´å½¢ã—ã¦ãƒ‘ã‚¹ã‚’å–å¾—
            log_path = finalize_log_and_get_path()
            
            if log_path and log_path.exists():
                # lecture_assets ãƒã‚±ãƒƒãƒˆã® logs ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
                # uid/lecture_id/logs/log.json ã¨ã—ã¦ä¿å­˜ã™ã‚‹ã®ãŒãŠã™ã™ã‚
                # (uidã¯tryãƒ–ãƒ­ãƒƒã‚¯å†…ã§å–å¾—ã—ã¦ã„ã‚‹ã®ã§ã€å¿µã®ç‚ºã“ã“ã§å†å–å¾—ã™ã‚‹ã‹ã€å¤‰æ•°ã®ã‚¹ã‚³ãƒ¼ãƒ—ã«æ³¨æ„)
                
                # tryãƒ–ãƒ­ãƒƒã‚¯ã®å¤–ã§ uid ãŒæœªå®šç¾©ã®å ´åˆã®å®‰å…¨ç­–
                # (Jobå–å¾—å‰ã«ã‚³ã‚±ãŸå ´åˆã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ããªã„ãŒã€ãã“ã¯è¨±å®¹ç¯„å›²)
                if 'uid' in locals() and 'lecture_id' in locals():
                    _upload_artifact(supabase, uid, lecture_id, log_path, "log.json", isLog=True)
                    print(f"ğŸ“ Log uploaded to Storage: logs/log.json")
                else:
                    print("âš ï¸ Could not upload log: uid or lecture_id not set.")
                    
        except Exception as log_e:
            print(f"âš ï¸ Failed to upload log file: {log_e}")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— (Cloud Runã®å®¹é‡ç¢ºä¿)
        if work_dir.exists():
            shutil.rmtree(work_dir)


# --- Helper Functions (ã‚³ãƒ¼ãƒ‰ã‚’è¦‹ã‚„ã™ãã™ã‚‹ãŸã‚ã®é“å…·) ---

def _update_job_progress(supabase, job_id: str, status: JobStatus, step_name: str, artifacts: dict):
    """
    DBã®é€²æ—çŠ¶æ³ã‚’æ›´æ–°ã™ã‚‹ã€‚
    step_name ã‹ã‚‰è‡ªå‹•çš„ã« step_number ã‚’å‰²ã‚Šå‡ºã™ã€‚
    """
    step_number = PIPELINE_STEPS_NUM.get(step_name, 0)
    
    print_log(f"ğŸ”„ Progress: [{step_number}] {step_name} (Status: {status})")
    
    supabase.table("processing_jobs").update({
        "status": status,
        "current_step": step_name,
        "step_number": step_number,
        "artifact_paths": artifacts, # æœ€æ–°ã®æˆæœç‰©ãƒ‘ã‚¹ãƒªã‚¹ãƒˆã§ä¸Šæ›¸ãæ›´æ–°
        "updated_at": datetime.now().isoformat()
    }).eq("id", job_id).execute()

def _upload_artifact(supabase, uid, lecture_id: str, local_path: Path, filename: str, isTemp: bool = False, isLog: bool = False) -> str:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«ã®ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’Supabase Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ãã®ãƒ‘ã‚¹ã‚’è¿”ã™ã€‚
    """
    storage_path = f"{uid}/{lecture_id}/artifacts/{filename}"
    if isTemp:
        storage_path = f"{uid}/{lecture_id}/artifacts/temp/{filename}"
    if isLog:
        storage_path = f"{uid}/{lecture_id}/log/{filename}"
    bucket_name = "lecture_assets"

    with open(local_path, "rb") as f:
        supabase.storage.from_(bucket_name).upload(
            path=storage_path,
            file=f,
            file_options={"upsert": "true"}
        )
    
    return f"{bucket_name}/{storage_path}"