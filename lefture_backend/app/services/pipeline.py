import shutil
import traceback
import json
from pathlib import Path
from datetime import datetime

# ä½œæˆã—ãŸè¨­å®šã¨Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
from app.core.config import JobStatus, PipelineSteps, PIPELINE_STEPS_NUM, BASE_WORK_DIR
from app.core.supabase import get_supabase_client
from app.services.helpers.llm_unified import UnifiedLLM, CostCollector

from app.services.logic.transcription import TranscriptionService
from app.services.logic.sentence_review import SentenceReviewService


async def run_lecture_pipeline(job_id: str):
    """
    ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œã•ã‚Œã‚‹ãƒ¡ã‚¤ãƒ³å‡¦ç†ã€‚
    processing_jobs ãƒ†ãƒ¼ãƒ–ãƒ«ã® job_id ã‚’å—ã‘å–ã‚Šã€æœ€å¾Œã¾ã§å‡¦ç†ã‚’è¡Œã†ã€‚
    """
    supabase = get_supabase_client()
    
    # ä½œæ¥­ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ (/tmp/job_id/)
    work_dir = BASE_WORK_DIR / job_id
    if work_dir.exists():
        shutil.rmtree(work_dir)
    work_dir.mkdir(parents=True, exist_ok=True)

    # å…±é€šãƒ„ãƒ¼ãƒ«ã®åˆæœŸåŒ–
    llm = UnifiedLLM(provider="gemini") # å¿…è¦ã«å¿œã˜ã¦ openai ã«å¤‰æ›´
    collector = CostCollector()
    
    # æˆæœç‰©ã®ãƒ‘ã‚¹ã‚’ä¸€æ™‚ä¿å­˜ã™ã‚‹è¾æ›¸
    current_artifacts = {}

    try:
        print(f"ğŸš€ Job Started: {job_id}")

        # ---------------------------------------------------------
        # 0. Jobãƒ‡ãƒ¼ã‚¿ã®å–å¾— & Lectureæƒ…å ±ã®ç¢ºèª
        # ---------------------------------------------------------
        # Jobæƒ…å ±ã‚’å–å¾—
        job_res = supabase.table("processing_jobs").select("*").eq("id", job_id).single().execute()
        job_data = job_res.data
        lecture_id = job_data["lecture_id"]
        
        if job_data["status"] != JobStatus.PENDING or job_data["current_step"] != PipelineSteps.PENDING:
            print(f"Job is already executed. [Status: {job_data['status']}, Step: {job_data['current_step']}]")
            return

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ PROCESSING ã«å¤‰æ›´
        _update_job_progress(supabase, job_id, JobStatus.PROCESSING, "READY", current_artifacts)

        # Lectureãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
        lecture_res = supabase.table("lecture_assets").select("storage_path").eq("lecture_id", lecture_id).single().execute()
        storage_path = lecture_res.data["storage_path"]
        uid = storage_path.split("/", 1)[0]


        # ---------------------------------------------------------
        # 1. DOWNLOADING (éŸ³å£°ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰)
        # ---------------------------------------------------------
        step_name = PipelineSteps.DOWNLOADING
        _update_job_progress(supabase, job_id, JobStatus.PROCESSING, step_name, current_artifacts)
        
        local_audio_path = work_dir / "input_audio.m4a"
        
        # Supabase Storage ('lecture_assets') ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        with open(local_audio_path, "wb") as f:
            res = supabase.storage.from_("lecture_assets").download(storage_path)
            f.write(res)
            
        print(f"âœ… Downloaded: {local_audio_path}")


        # ---------------------------------------------------------
        # 2. TRANSCRIBING (æ–‡å­—èµ·ã“ã—)
        # ---------------------------------------------------------
        step_name = PipelineSteps.TRANSCRIBING
        _update_job_progress(supabase, job_id, JobStatus.PROCESSING, step_name, current_artifacts)

        transcriber = TranscriptionService(collector)
        transcript_path = transcriber.run(local_audio_path, work_dir)
        
        # æˆæœç‰©ã‚’Supabase Storageã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼†ãƒ‘ã‚¹è¨˜éŒ²
        remote_trans_path = _upload_artifact(supabase, uid, lecture_id, transcript_path, "transcript.json")
        current_artifacts["transcript_json"] = remote_trans_path


        # ---------------------------------------------------------
        # 3. SENTENCE_REVIEWING (æ–‡ç« æ ¡æ­£)
        # ---------------------------------------------------------
        step_name = PipelineSteps.SENTENCE_REVIEWING
        _update_job_progress(supabase, job_id, JobStatus.PROCESSING, step_name, current_artifacts)

        reviewer = SentenceReviewService(llm, collector)
        reviewed_paths = reviewer.run(transcript_path, work_dir)
        
        final_json_path = None

        for path in reviewed_paths:
            # ãƒ•ã‚¡ã‚¤ãƒ«åã§åˆ¤æ–­ã—ã¦æŒ¯ã‚Šåˆ†ã‘ã‚‹
            filename = path.name
            
            if filename == "reviewed_sentences_raw.json":
                # ã“ã‚Œã¯ã€Œé€”ä¸­çµŒé (Temp)ã€
                remote_path = _upload_artifact(supabase, uid, lecture_id, path, filename, isTemp=True)
                current_artifacts["reviewed_sentences_raw_json"] = remote_path
            
            elif filename == "reviewed_sentences.json":
                # ã“ã‚ŒãŒã€Œå®Œæˆå“ã€
                remote_path = _upload_artifact(supabase, uid, lecture_id, path, filename)
                current_artifacts["reviewed_sentences_json"] = remote_path
                final_json_path = path
                
            elif filename == "reviewed_sentences_raw_text.txt":
                # ã“ã‚Œã¯ã€Œå¤±æ•—æ™‚ã®ãƒ­ã‚° (Temp)ã€
                remote_path = _upload_artifact(supabase, uid, lecture_id, path, filename, isTemp=True)
                current_artifacts["reviewed_sentences_error_text"] = remote_path

        # ã‚‚ã—å®Œæˆå“(final_json_path)ãŒãªã‘ã‚Œã°ã€ã“ã“ã§ã‚¨ãƒ©ãƒ¼ã«ã™ã‚‹
        if not final_json_path:
            raise ValueError("Sentence Review failed to generate final JSON. Check temp artifacts for raw text.")


        # ---------------------------------------------------------
        # X. COMPLETED (å®Œäº†å‡¦ç†)
        # ---------------------------------------------------------
        step_name = PipelineSteps.COMPLETED
        _update_job_progress(supabase, job_id, JobStatus.DONE, step_name, current_artifacts)
        
        # æœ€å¾Œã« lectures ãƒ†ãƒ¼ãƒ–ãƒ«ã® final_markdown_path ãªã©ã‚’æ›´æ–°ã—ã¦ã‚‚è‰¯ã„
        # supabase.table("lectures").update({...}).eq("id", lecture_id).execute()

        print(f"ğŸ‰ Job Completed Successfully: {job_id}")
        print(collector.report())

    except Exception as e:
        # ---------------------------------------------------------
        # ERROR HANDLING (å¤±æ•—æ™‚ã®å‡¦ç†)
        # ---------------------------------------------------------
        error_msg = f"{str(e)}\n{traceback.format_exc()}"
        print(f"âŒ Job Failed at {step_name}: {error_msg}")
        
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä½œæˆ
        error_data = {
            "message": str(error_msg),
            "step": step_name,
            "timestamp": datetime.now().isoformat(),
            "traceback": traceback.format_exc()
        }

        # DBæ›´æ–°: Status=ERROR, Step=å¤±æ•—ã—ãŸã‚¹ãƒ†ãƒƒãƒ—ã®ã¾ã¾
        supabase.table("processing_jobs").update({
            "status": JobStatus.ERROR,
            "error_message": json.dumps(error_data), # JSONBå¯¾å¿œ
            "updated_at": datetime.now().isoformat()
        }).eq("id", job_id).execute()

    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— (Cloud Runã®å®¹é‡ç¢ºä¿)
        if work_dir.exists():
            shutil.rmtree(work_dir)


# --- Helper Functions (ã‚³ãƒ¼ãƒ‰ã‚’è¦‹ã‚„ã™ãã™ã‚‹ãŸã‚ã®é“å…·) ---

def _update_job_progress(supabase, job_id: str, status: JobStatus, step_name: str, artifacts: dict):
    """
    DBã®é€²æ—çŠ¶æ³ã‚’æ›´æ–°ã™ã‚‹ã€‚
    step_name ã‹ã‚‰è‡ªå‹•çš„ã« step_number ã‚’å‰²ã‚Šå‡ºã™ã€‚
    """
    step_number = PIPELINE_STEPS_NUM.get(step_name, 0)
    
    print(f"ğŸ”„ Progress: [{step_number}] {step_name} (Status: {status})")
    
    supabase.table("processing_jobs").update({
        "status": status,
        "current_step": step_name,
        "step_number": step_number,
        "artifact_paths": artifacts, # æœ€æ–°ã®æˆæœç‰©ãƒ‘ã‚¹ãƒªã‚¹ãƒˆã§ä¸Šæ›¸ãæ›´æ–°
        "updated_at": datetime.now().isoformat()
    }).eq("id", job_id).execute()

def _upload_artifact(supabase, uid, lecture_id: str, local_path: Path, filename: str, isTemp: bool = False) -> str:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«ã®ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’Supabase Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ãã®ãƒ‘ã‚¹ã‚’è¿”ã™ã€‚
    """
    storage_path = f"{uid}/{lecture_id}/artifacts/{filename}"
    if isTemp:
        storage_path = f"{uid}/{lecture_id}/artifacts/temp/{filename}"
    bucket_name = "lecture_assets"

    with open(local_path, "rb") as f:
        supabase.storage.from_(bucket_name).upload(
            path=storage_path,
            file=f,
            file_options={"upsert": "true"}
        )
    
    return f"{bucket_name}/{storage_path}"